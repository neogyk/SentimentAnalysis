{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.idea', 'data', 'imdb-small.ipynb', 'model-take-04', 'model-take-05', 'submission-3', 'test', 'tokenizer.pkl', 'w2v']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-44bdcac5dce8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./data/imdb_small.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"|\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/imdb_small.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0      0  Once again Mr. Costner has dragged out a movie...\n",
      "1      0  This is an example of why the majority of acti...\n",
      "2      0  First of all I hate those moronic rappers, who...\n",
      "3      0  Not even the Beatles could write songs everyon...\n",
      "4      0  Brass pictures (movies is not a fitting word f...\n"
     ]
    }
   ],
   "source": [
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    # 1. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n",
    "    #\n",
    "    # 2. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 3. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in df[\"text\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 10    # Word vector dimensionality                      \n",
    "min_word_count = 5   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2017-03-13 19:08:24,310 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2017-03-13 19:08:25,390 : INFO : collecting all words and their counts\n",
      "2017-03-13 19:08:25,392 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-03-13 19:08:25,461 : INFO : PROGRESS: at sentence #10000, processed 212704 words, keeping 14820 word types\n",
      "2017-03-13 19:08:25,517 : INFO : PROGRESS: at sentence #20000, processed 431438 words, keeping 21322 word types\n",
      "2017-03-13 19:08:25,651 : INFO : PROGRESS: at sentence #30000, processed 643687 words, keeping 25985 word types\n",
      "2017-03-13 19:08:25,779 : INFO : PROGRESS: at sentence #40000, processed 856767 words, keeping 29858 word types\n",
      "2017-03-13 19:08:25,873 : INFO : PROGRESS: at sentence #50000, processed 1077540 words, keeping 33390 word types\n",
      "2017-03-13 19:08:25,973 : INFO : PROGRESS: at sentence #60000, processed 1286257 words, keeping 35998 word types\n",
      "2017-03-13 19:08:26,068 : INFO : PROGRESS: at sentence #70000, processed 1498920 words, keeping 38580 word types\n",
      "2017-03-13 19:08:26,149 : INFO : PROGRESS: at sentence #80000, processed 1716255 words, keeping 41041 word types\n",
      "2017-03-13 19:08:26,276 : INFO : PROGRESS: at sentence #90000, processed 1931149 words, keeping 43282 word types\n",
      "2017-03-13 19:08:26,369 : INFO : PROGRESS: at sentence #100000, processed 2140481 words, keeping 45312 word types\n",
      "2017-03-13 19:08:26,439 : INFO : PROGRESS: at sentence #110000, processed 2359054 words, keeping 47434 word types\n",
      "2017-03-13 19:08:26,505 : INFO : PROGRESS: at sentence #120000, processed 2576929 words, keeping 49354 word types\n",
      "2017-03-13 19:08:26,569 : INFO : PROGRESS: at sentence #130000, processed 2795221 words, keeping 51287 word types\n",
      "2017-03-13 19:08:26,644 : INFO : PROGRESS: at sentence #140000, processed 3019435 words, keeping 53138 word types\n",
      "2017-03-13 19:08:26,721 : INFO : PROGRESS: at sentence #150000, processed 3247042 words, keeping 55179 word types\n",
      "2017-03-13 19:08:26,785 : INFO : PROGRESS: at sentence #160000, processed 3470865 words, keeping 56992 word types\n",
      "2017-03-13 19:08:26,863 : INFO : PROGRESS: at sentence #170000, processed 3705403 words, keeping 58735 word types\n",
      "2017-03-13 19:08:26,930 : INFO : PROGRESS: at sentence #180000, processed 3931225 words, keeping 60415 word types\n",
      "2017-03-13 19:08:27,001 : INFO : PROGRESS: at sentence #190000, processed 4155649 words, keeping 61880 word types\n",
      "2017-03-13 19:08:27,069 : INFO : PROGRESS: at sentence #200000, processed 4383829 words, keeping 63354 word types\n",
      "2017-03-13 19:08:27,145 : INFO : PROGRESS: at sentence #210000, processed 4615085 words, keeping 64860 word types\n",
      "2017-03-13 19:08:27,224 : INFO : PROGRESS: at sentence #220000, processed 4847022 words, keeping 66266 word types\n",
      "2017-03-13 19:08:27,311 : INFO : PROGRESS: at sentence #230000, processed 5073859 words, keeping 67776 word types\n",
      "2017-03-13 19:08:27,382 : INFO : PROGRESS: at sentence #240000, processed 5291681 words, keeping 68944 word types\n",
      "2017-03-13 19:08:27,459 : INFO : PROGRESS: at sentence #250000, processed 5524349 words, keeping 70344 word types\n",
      "2017-03-13 19:08:27,525 : INFO : PROGRESS: at sentence #260000, processed 5756442 words, keeping 71615 word types\n",
      "2017-03-13 19:08:27,590 : INFO : PROGRESS: at sentence #270000, processed 5978762 words, keeping 72866 word types\n",
      "2017-03-13 19:08:27,651 : INFO : PROGRESS: at sentence #280000, processed 6189234 words, keeping 73953 word types\n",
      "2017-03-13 19:08:27,713 : INFO : PROGRESS: at sentence #290000, processed 6402175 words, keeping 75080 word types\n",
      "2017-03-13 19:08:27,777 : INFO : PROGRESS: at sentence #300000, processed 6618122 words, keeping 76216 word types\n",
      "2017-03-13 19:08:27,838 : INFO : PROGRESS: at sentence #310000, processed 6828842 words, keeping 77288 word types\n",
      "2017-03-13 19:08:27,901 : INFO : PROGRESS: at sentence #320000, processed 7043983 words, keeping 78274 word types\n",
      "2017-03-13 19:08:27,966 : INFO : PROGRESS: at sentence #330000, processed 7259592 words, keeping 79305 word types\n",
      "2017-03-13 19:08:28,028 : INFO : PROGRESS: at sentence #340000, processed 7476027 words, keeping 80487 word types\n",
      "2017-03-13 19:08:28,095 : INFO : PROGRESS: at sentence #350000, processed 7696365 words, keeping 81614 word types\n",
      "2017-03-13 19:08:28,159 : INFO : PROGRESS: at sentence #360000, processed 7913332 words, keeping 82541 word types\n",
      "2017-03-13 19:08:28,221 : INFO : PROGRESS: at sentence #370000, processed 8121779 words, keeping 83518 word types\n",
      "2017-03-13 19:08:28,282 : INFO : PROGRESS: at sentence #380000, processed 8335157 words, keeping 84368 word types\n",
      "2017-03-13 19:08:28,342 : INFO : PROGRESS: at sentence #390000, processed 8548912 words, keeping 85298 word types\n",
      "2017-03-13 19:08:28,407 : INFO : PROGRESS: at sentence #400000, processed 8766800 words, keeping 86081 word types\n",
      "2017-03-13 19:08:28,475 : INFO : PROGRESS: at sentence #410000, processed 8996203 words, keeping 87154 word types\n",
      "2017-03-13 19:08:28,556 : INFO : PROGRESS: at sentence #420000, processed 9230182 words, keeping 88171 word types\n",
      "2017-03-13 19:08:28,807 : INFO : PROGRESS: at sentence #430000, processed 9460285 words, keeping 89187 word types\n",
      "2017-03-13 19:08:28,947 : INFO : PROGRESS: at sentence #440000, processed 9684392 words, keeping 90314 word types\n",
      "2017-03-13 19:08:29,026 : INFO : PROGRESS: at sentence #450000, processed 9911632 words, keeping 91228 word types\n",
      "2017-03-13 19:08:29,130 : INFO : PROGRESS: at sentence #460000, processed 10143304 words, keeping 92201 word types\n",
      "2017-03-13 19:08:29,244 : INFO : PROGRESS: at sentence #470000, processed 10376139 words, keeping 93319 word types\n",
      "2017-03-13 19:08:29,358 : INFO : PROGRESS: at sentence #480000, processed 10610667 words, keeping 94333 word types\n",
      "2017-03-13 19:08:29,452 : INFO : PROGRESS: at sentence #490000, processed 10840402 words, keeping 95252 word types\n",
      "2017-03-13 19:08:29,547 : INFO : PROGRESS: at sentence #500000, processed 11071796 words, keeping 96199 word types\n",
      "2017-03-13 19:08:29,919 : INFO : PROGRESS: at sentence #510000, processed 11300517 words, keeping 96989 word types\n",
      "2017-03-13 19:08:30,351 : INFO : PROGRESS: at sentence #520000, processed 11526881 words, keeping 97972 word types\n",
      "2017-03-13 19:08:30,727 : INFO : PROGRESS: at sentence #530000, processed 11760730 words, keeping 98894 word types\n",
      "2017-03-13 19:08:30,791 : INFO : collected 99426 word types from a corpus of 11911303 raw words and 536630 sentences\n",
      "2017-03-13 19:08:30,804 : INFO : Loading a fresh vocabulary\n",
      "2017-03-13 19:08:31,037 : INFO : min_count=5 retains 39282 unique words (39% of original 99426, drops 60144)\n",
      "2017-03-13 19:08:31,038 : INFO : min_count=5 leaves 11812894 word corpus (99% of original 11911303, drops 98409)\n",
      "2017-03-13 19:08:31,620 : INFO : deleting the raw counts dictionary of 99426 items\n",
      "2017-03-13 19:08:32,875 : INFO : sample=0.001 downsamples 49 most-common words\n",
      "2017-03-13 19:08:32,877 : INFO : downsampling leaves estimated 8763521 word corpus (74.2% of prior 11812894)\n",
      "2017-03-13 19:08:32,879 : INFO : estimated required memory for 39282 words and 100 dimensions: 51066600 bytes\n",
      "2017-03-13 19:08:33,787 : INFO : resetting layer weights\n",
      "2017-03-13 19:08:35,720 : INFO : training model with 4 workers on 39282 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-03-13 19:08:35,865 : INFO : expecting 536630 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-03-13 19:08:37,955 : INFO : PROGRESS: at 1.06% examples, 446090 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:08:38,961 : INFO : PROGRESS: at 3.60% examples, 757299 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:08:39,966 : INFO : PROGRESS: at 6.10% examples, 868063 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:08:40,972 : INFO : PROGRESS: at 8.62% examples, 933385 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:08:41,974 : INFO : PROGRESS: at 11.06% examples, 959084 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:08:42,986 : INFO : PROGRESS: at 13.53% examples, 972564 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-13 19:08:43,993 : INFO : PROGRESS: at 16.01% examples, 987877 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:08:45,000 : INFO : PROGRESS: at 18.31% examples, 993741 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:08:46,008 : INFO : PROGRESS: at 20.55% examples, 993320 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-13 19:08:47,011 : INFO : PROGRESS: at 22.92% examples, 994374 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:08:48,013 : INFO : PROGRESS: at 25.45% examples, 1002808 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:08:49,019 : INFO : PROGRESS: at 27.72% examples, 1003581 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:08:50,019 : INFO : PROGRESS: at 30.20% examples, 1011373 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:08:51,023 : INFO : PROGRESS: at 32.76% examples, 1016164 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:08:52,031 : INFO : PROGRESS: at 35.18% examples, 1016973 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:08:53,041 : INFO : PROGRESS: at 37.63% examples, 1022489 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-13 19:08:54,045 : INFO : PROGRESS: at 40.11% examples, 1028048 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:08:55,051 : INFO : PROGRESS: at 42.59% examples, 1029009 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:08:56,054 : INFO : PROGRESS: at 45.02% examples, 1029335 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-13 19:08:57,062 : INFO : PROGRESS: at 47.28% examples, 1028020 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-13 19:08:58,067 : INFO : PROGRESS: at 49.51% examples, 1026622 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:08:59,067 : INFO : PROGRESS: at 52.01% examples, 1028411 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:09:00,070 : INFO : PROGRESS: at 54.63% examples, 1032226 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:09:01,082 : INFO : PROGRESS: at 56.97% examples, 1032421 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-13 19:09:02,084 : INFO : PROGRESS: at 59.29% examples, 1033208 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:09:03,085 : INFO : PROGRESS: at 61.88% examples, 1036601 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:09:04,092 : INFO : PROGRESS: at 64.56% examples, 1039918 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:09:05,097 : INFO : PROGRESS: at 67.11% examples, 1043165 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:09:06,108 : INFO : PROGRESS: at 69.60% examples, 1045458 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-13 19:09:07,110 : INFO : PROGRESS: at 72.12% examples, 1046357 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:09:08,111 : INFO : PROGRESS: at 74.68% examples, 1047674 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:09:09,114 : INFO : PROGRESS: at 77.08% examples, 1048557 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:09:10,128 : INFO : PROGRESS: at 79.46% examples, 1049187 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:09:11,132 : INFO : PROGRESS: at 81.72% examples, 1046951 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:09:12,133 : INFO : PROGRESS: at 84.16% examples, 1046442 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:09:13,137 : INFO : PROGRESS: at 86.67% examples, 1048204 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:09:14,146 : INFO : PROGRESS: at 89.07% examples, 1048521 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:09:15,150 : INFO : PROGRESS: at 91.56% examples, 1049272 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:09:16,159 : INFO : PROGRESS: at 94.12% examples, 1050042 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:09:17,160 : INFO : PROGRESS: at 96.38% examples, 1048761 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-13 19:09:18,160 : INFO : PROGRESS: at 98.64% examples, 1048386 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-13 19:09:18,742 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-03-13 19:09:18,748 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-03-13 19:09:18,752 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-03-13 19:09:18,753 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-03-13 19:09:18,754 : INFO : training on 59556515 raw words (43816940 effective words) took 41.8s, 1048188 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model (this will take some time)\n",
    "\n",
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 19:16:10,612 : INFO : saving Word2Vec object under imdb_word2vec_nfeatures_10_nword_5_windows_10_downsampling_1e-3, separately None\n",
      "2017-03-13 19:16:10,617 : INFO : not storing attribute syn0norm\n",
      "2017-03-13 19:16:10,618 : INFO : not storing attribute cum_table\n",
      "2017-03-13 19:16:11,908 : INFO : saved imdb_word2vec_nfeatures_10_nword_5_windows_10_downsampling_1e-3\n"
     ]
    }
   ],
   "source": [
    "fname = \"imdb_word2vec_nfeatures_10_nword_5_windows_10_downsampling_1e-3\"\n",
    "model.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}